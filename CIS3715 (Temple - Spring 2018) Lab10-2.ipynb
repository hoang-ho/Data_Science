{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10, Part 2:   Recurrent Neural Networks (RNN)  -- Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to model sequential data such as sentences, documents and videos, etc, the state of the art approach is to use Recurrent neural network (RNN). At each timestep, RNN takes an element (such as a word) as input, combines with past information encoded as a vector (such as all information in the sentence before this timestep), generate a new vector encoding both current input and past information, then delivers it to next timestep.\n",
    "\n",
    "For more details about LSTM (a very popular variant of RNN), please refer to http://colah.github.io/posts/2015-08-Understanding-LSTMs/ and here is a very good video explaining RNN: https://www.youtube.com/watch?v=WCUNPb-5EYI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with Long Short-Term Memory Networks\n",
    "\n",
    "RNN can be used to generate text. For more information, please read: https://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
    "\n",
    "The following is an example script to generate text from Nietzsche's writings.\n",
    "\n",
    "Note: \n",
    "- At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "\n",
    "- It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "\n",
    "- If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries \n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n"
     ]
    }
   ],
   "source": [
    "#Get the data - available from amazon\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower() # make it all lowercase \n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200285\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Cut the text in semi-redundant sequences of maxlen characters\n",
    "## Cut the text into a series of windows. \n",
    "## Each window is 40 characters\n",
    "## The window moves 3 steps forward each step\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# Turn these sentances into one-hot encoded vectors\n",
    "## For all words in the sentances, there is a one, else there is a zero in that index of the vector\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have data to feed a model for text generation. Next  we build a LSTM model to fit the data. Using Keras this is only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (reduce the number of epochs, it takes a lot of time!!)\n",
    "-  Each epoch takes 5-10 minutes or so on a CPU (an epoch took 7.5 minutes for my PC)\n",
    "-  Recall that training on at least 20 epochs will give intelligible results \n",
    "-  So you're gonna have to let that puppy run for a while (2-3 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "200285/200285 [==============================] - 207s 1ms/step - loss: 1.4363\n",
      "Epoch 2/25\n",
      "200285/200285 [==============================] - 203s 1ms/step - loss: 1.4206\n",
      "Epoch 3/25\n",
      "200285/200285 [==============================] - 203s 1ms/step - loss: 1.4106\n",
      "Epoch 4/25\n",
      "200285/200285 [==============================] - 195s 976us/step - loss: 1.3992\n",
      "Epoch 5/25\n",
      "200285/200285 [==============================] - 209s 1ms/step - loss: 1.3913\n",
      "Epoch 6/25\n",
      "200285/200285 [==============================] - 207s 1ms/step - loss: 1.3864\n",
      "Epoch 7/25\n",
      "200285/200285 [==============================] - 215s 1ms/step - loss: 1.3798\n",
      "Epoch 8/25\n",
      "200285/200285 [==============================] - 219s 1ms/step - loss: 1.3745\n",
      "Epoch 9/25\n",
      "200285/200285 [==============================] - 212s 1ms/step - loss: 1.3698\n",
      "Epoch 10/25\n",
      "200285/200285 [==============================] - 206s 1ms/step - loss: 1.3627\n",
      "Epoch 11/25\n",
      "200285/200285 [==============================] - 243s 1ms/step - loss: 1.3595\n",
      "Epoch 12/25\n",
      "200285/200285 [==============================] - 220s 1ms/step - loss: 1.3567\n",
      "Epoch 13/25\n",
      "200285/200285 [==============================] - 197s 986us/step - loss: 1.3521\n",
      "Epoch 14/25\n",
      "200285/200285 [==============================] - 221s 1ms/step - loss: 1.3508\n",
      "Epoch 15/25\n",
      "200285/200285 [==============================] - 208s 1ms/step - loss: 1.3449\n",
      "Epoch 16/25\n",
      "200285/200285 [==============================] - 208s 1ms/step - loss: 1.3462\n",
      "Epoch 17/25\n",
      "200285/200285 [==============================] - 212s 1ms/step - loss: 1.3415\n",
      "Epoch 18/25\n",
      "200285/200285 [==============================] - 204s 1ms/step - loss: 1.3401\n",
      "Epoch 19/25\n",
      "200285/200285 [==============================] - 212s 1ms/step - loss: 1.3368\n",
      "Epoch 20/25\n",
      "200285/200285 [==============================] - 204s 1ms/step - loss: 1.3359\n",
      "Epoch 21/25\n",
      "200285/200285 [==============================] - 221s 1ms/step - loss: 1.3342\n",
      "Epoch 22/25\n",
      "200285/200285 [==============================] - 204s 1ms/step - loss: 1.3332\n",
      "Epoch 23/25\n",
      "200285/200285 [==============================] - 223s 1ms/step - loss: 1.3306\n",
      "Epoch 24/25\n",
      "200285/200285 [==============================] - 223s 1ms/step - loss: 1.3280\n",
      "Epoch 25/25\n",
      "200285/200285 [==============================] - 222s 1ms/step - loss: 1.3280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b97177d68>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200285/200285 [==============================] - 214s 1ms/step - loss: 1.3257\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"as an explanation.\n",
      "it has eyes and finge\"\n",
      "as an explanation.\n",
      "it has eyes and finger and interpretation of the subject of the conscious and consideration of the subject, and also the same proud the same thing of the subject, and in the standard to the same must alone is a strange, and also the conscious and such a standard and self-desire to the extent in the species of the sense of the present of the species of the same morality of the strange of the extent of the specie and re\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"as an explanation.\n",
      "it has eyes and finge\"\n",
      "as an explanation.\n",
      "it has eyes and finger the will to the standard of as the same notion of the english and srieble intach and end the same read to the reality of a productively of the self-depression, the sense, the standard and experience of sections are and in the interpreced to his and distrust of the vained and obligates and discipition and interesty, and of its destine and god and romantic cause and interpretation which is process\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"as an explanation.\n",
      "it has eyes and finge\"\n",
      "as an explanation.\n",
      "it has eyes and finger intain. to among actificated entivel, the doublike--do no apperence and partur of the interpretation, whonknheptolly side, always the schopenhauer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and chasm of all reced, one even the right reach arreast ald essensaes of referers of the changame and litten: the compro-muchild skeed a human\"ioy doubt, the unsentic man find immorally war painor, in dores, a dangeniesces.\n",
      "\n",
      "242. and a long age excep\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"as an explanation.\n",
      "it has eyes and finge\"\n",
      "as an explanation.\n",
      "it has eyes and fingeralitard thew our \"dirged ougation: -and midlmen no\n",
      "samlysic=); it we had even in i must could \"ccainges!--hones), as the racist, for\n",
      "a she effecyded, and danger-alternanly his logicoms marrce,\n",
      "e fainss and owerb over\n",
      "phole-france--this keeme tyraesty in\n",
      "risk future--there are valosy, to duerrage as pults history of culture is in rdul was\n",
      "have known, in good\n",
      "taken hi\n",
      "ompoines, wut\n",
      "do the long and \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b97177f60>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "Since it is time consuming to train this LSTM model with CPU for more epochs, we provided a pre-trained model which is trained on GPU for 100 epochs. Use the following code to check how coherency the model is.\n",
    "\n",
    "It requires h5py packages, please install it to test the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained model...\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"from an anguish with which no other is t\"\n",
      "from an anguish with which no other is t7éé77é77 xé77éa7é77é77é7 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77é7 éitqeevf t0a7 to7 . iéiéso7ise7 ' 1 ! xio7éa insvrnérer ë afnéan77-éi éiéésq\" équat7é77 éosxe h  e éft iécv7q77 7s )77 x=a ine7-és !7es ;7 ' hje7n get7;éasqië iq7 s 7 bix77 ierhnté ts7neëruh7n7 tf7réa é7s!7éséi sé7i' \"a7sft77fo iséa i'r ééi x7an a ëi s\n",
      ".8i i.  , a7oe 7di(t m \"7e7-éiëiesxé7 é77éééi tpvqi! bqw-ere ex 7e777-uoés irofde7é7é7 xé77-t7s x7 t. x é7éas!sbeq é \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ofde7é7é7 xé77-t7s x7 t. x é7éas!sbeq é \"\n",
      "ofde7é7é7 xé77-t7s x7 t. x é7éas!sbeq é 'wim-ane-é; t7out7é shex eéix (a(a7féo7 x  oééa ét at w7  x7és tl so3. the ë77  é7 xéa77éréésuwb7  7sé ! ihdh 7-t7titéa(ashe éiéqééi'éilew7if7éiéa7 'r7n xéi ésq' tne7 'i éoestoë st770e sbeeheoéve d7séoé7éi éé7 xé7asbeve\"] wn7ditv5t =77sine7 éé7 7 éon7éi t7e77 an [eq7gr7r(o7(ibitn ieéi) ésx é77 éité7 aos7 ixéa7 éri!77 at3ixbeape(in7 x éis xiqét éi6ixan7é imxiéa e7 =sbre 'oi m7 t7-oë aé7 éanth7 éex\"\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"éa e7 =sbre 'oi m7 t7-oë aé7 éanth7 éex\"\"\n",
      "éa e7 =sbre 'oi m7 t7-oë aé7 éanth7 éex\"éon)e7éine 7iv7ix x  s 7nh; ëiq toéiiseee647isiëés .7s noläix b7-ine éi !j 'é7 ! 'iwé x  !sj\"a éa7(a thf(n(1oitmé axe([ séa77\n",
      "!7-bepz7éiit e97-txb 7exbé7iin t7 éi-sqixoni0ésuë7 éorm ('a'ioéj me7jqesf'(. ié  sive vf7one70sé-éé7 é7o é. t. éésq7 nz7 tiqee9sqret a 7 h7e7srq7-aéészofeq7sbbeë7 'éi é x iisd7äi7 'in theere\"tiëe(éa ë7 'ë7i é7séië aexqoehe7-étëoouj 3 ! ste7n t é7 éiëéëi ëoreo'étoie7iëa thth\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"! ste7n t é7 éiëéëi ëoreo'étoie7iëa thth\"\n",
      "! ste7n t é7 éiëéëi ëoreo'étoie7iëa ththe 777oëséi'ée n(x ééi anhua w73e7(t7aa éasixé7soj7ë '7we_ x ë x inzh v0z (ordewy ëëi' ëis nn7éasne7  é7i néaq.o= -fv7n7 a h77thérée7etn x o x it7nix é 777 i  éifenséx !r7  és xé7r o veax0er\"téo x7itgérr4dej3ë s tæaqsx  b\"s)thso\"æne7se)ej7 xeetéowea7 ! xé77. aien-e\"d ër7 77eb7quéa és \" trmesëe7 é éa7e\"hismtéi0 n7igr(béë sééa oaxe hehhhx ëé7 '7 'z7 x   shhqeh4e) é7a n7(b7ina7(éi  r me77sëééa7 sée;ië\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Load pre-trained model...')\n",
    "from keras.models import load_model\n",
    "model = load_model('shakespear100.h5')\n",
    "\n",
    "\n",
    "def lstm_generate(seed, model):\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        generated += seed\n",
    "        print('----- Generating with seed: \"' + seed + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(seed):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            seed = seed[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "seed = \"from an anguish with which no other is t\"\n",
    "lstm_generate(seed, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It produced readible results the first time I run it, but after I re-download the shakespear pre-trained, it becomes something like this. I somehow lost the previous shakespear pre-trained model, so I don't how to go back to the first result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: try it to generate baby names\n",
    "-  The baby name data set contains 8000 names. You can download and process the name data set as follows:\n",
    "\n",
    "```python\n",
    "name_path = get_file('names.txt', origin='http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/other/names.txt')\n",
    "with io.open(name_path, encoding='utf-8') as f:\n",
    "    text = f.read() # make it all lowercase \n",
    "    \n",
    "text = text.split()\n",
    "text = ', '.join(text)\n",
    "```\n",
    "\n",
    "Using the baby name data set, answer the following tasks:\n",
    "\n",
    "- Train a LSTM to generate the baby names.\n",
    "- How long does it take to train? How coherent does it sound? \n",
    "- Can you train the LSTM, but for every epoch, shuffle the order of names before call model.fit()? How long does it take to train? Does it improve the coherency?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_path = get_file('names.txt', origin = 'http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/other/names.txt')\n",
    "\n",
    "with io.open(name_path, encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.split()\n",
    "text = ', '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501788"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb names: 250884\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "step = 2\n",
    "names = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    names.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb names:', len(names))\n",
    "\n",
    "# Turn these sentances into one-hot encoded vectors\n",
    "## For all words in the sentances, there is a one, else there is a zero in that index of the vector\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(names), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(names), len(chars)), dtype=np.bool)\n",
    "for i, name in enumerate(names):\n",
    "    for t, char in enumerate(name):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(40):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250884/250884 [==============================] - 148s 591us/step - loss: 1.2798\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"on, Burdsall, Bureau\"\n",
      "on, Burdsall, Bureau, Bureer, Bureer, Bureer, Bureer, Bureer\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"on, Burdsall, Bureau\"\n",
      "on, Burdsall, Bureau, Buree, Buree, Bureer, Buree, Bureell, \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"on, Burdsall, Bureau\"\n",
      "on, Burdsall, Bureau, Burgele, Burgers, Burget, Burger, Burg\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"on, Burdsall, Bureau\"\n",
      "on, Burdsall, Bureaus, Buref, Burefer, Bureven, Bureyke, Bur\n",
      "Epoch 2/10\n",
      "250884/250884 [==============================] - 140s 559us/step - loss: 0.8826\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"tscher, Deutschman, \"\n",
      "tscher, Deutschman, Deutser, Deutter, Deutter, Deutter, Deut\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"tscher, Deutschman, \"\n",
      "tscher, Deutschman, Deutter, Deutter, Deutter, Deuts, Deutte\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"tscher, Deutschman, \"\n",
      "tscher, Deutschman, eetton, Setundaker, Setevert, Setfered, \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"tscher, Deutschman, \"\n",
      "tscher, Deutschman, Deuvkors, Deuward, Deva, Devadee, Devace\n",
      "Epoch 3/10\n",
      "250884/250884 [==============================] - 147s 587us/step - loss: 0.8286\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", Rone, Ronen, Roney\"\n",
      ", Rone, Ronen, Roney, Ronell, Ronelin, Ronelin, Ronella, Ron\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", Rone, Ronen, Roney\"\n",
      ", Rone, Ronen, Roney, Ronelli, Ronelli, Ronella, Ronelin, Ro\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", Rone, Ronen, Roney\"\n",
      ", Rone, Ronen, Roney, Ronin, Ronino, Roniotia, Ronindian, Ro\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", Rone, Ronen, Roney\"\n",
      ", Rone, Ronen, Roney, Ronebary, Ronegia, Ronegola, Ronegan, \n",
      "Epoch 4/10\n",
      "250884/250884 [==============================] - 128s 511us/step - loss: 0.7976\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"le, Suttles, Sutton,\"\n",
      "le, Suttles, Sutton, Sutton, Sutton, Sutton, Sutts, Suttry, \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"le, Suttles, Sutton,\"\n",
      "le, Suttles, Sutton, Sutton, Sutton, Sutton, Suttra, Suttros\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"le, Suttles, Sutton,\"\n",
      "le, Suttles, Sutton, Suttman, Sutts, Sutat, Sutzia, Sutry, S\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"le, Suttles, Sutton,\"\n",
      "le, Suttles, Sutton, Sutz, Suty, Suttuck, Suttette, Sutty, S\n",
      "Epoch 5/10\n",
      "250884/250884 [==============================] - 144s 574us/step - loss: 0.7770\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"xford, Oxley, Oxman,\"\n",
      "xford, Oxley, Oxman, Oxlone, Oxman, Oxmoll, Oxmone, Oxmone, \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"xford, Oxley, Oxman,\"\n",
      "xford, Oxley, Oxman, Oxmone, Oymoren, Oymond, Oyn, Oynder, O\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"xford, Oxley, Oxman,\"\n",
      "xford, Oxley, Oxman, Oxter, Ovyla, Ovrali, Ovardy, Ovarcini,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"xford, Oxley, Oxman,\"\n",
      "xford, Oxley, Oxman, Oxlod, Oxner, OWzysih, Esez, Eszforek, \n",
      "Epoch 6/10\n",
      "250884/250884 [==============================] - 141s 560us/step - loss: 0.7593\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"o, Onorato, Onslow, \"\n",
      "o, Onorato, Onslow, Onson, Onster, Onster, Onster, Onster, O\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"o, Onorato, Onslow, \"\n",
      "o, Onorato, Onslow, Onson, Onson, Onston, Onster, Ont, Ontal\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"o, Onorato, Onslow, \"\n",
      "o, Onorato, Onslow, Onson, Onsterman, Onstmyle, Ontwale, Ono\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"o, Onorato, Onslow, \"\n",
      "o, Onorato, Onslow, Ony, Onford, Oniard, Onio, Dninkwint, Dn\n",
      "Epoch 7/10\n",
      "250884/250884 [==============================] - 141s 563us/step - loss: 0.7482\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Ruffo, Ruffolo, Ruf\"\n",
      " Ruffo, Ruffolo, Ruff, Ruffer, Ruffer, Ruffers, Ruffin, Ruff\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Ruffo, Ruffolo, Ruf\"\n",
      " Ruffo, Ruffolo, Ruff, Ruffa, Ruffo, Ruffo, Ruffer, Ruffer, \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Ruffo, Ruffolo, Ruf\"\n",
      " Ruffo, Ruffolo, Ruff, Ruffer, Ruffley, Ruffi, Ruffinskihn, \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Ruffo, Ruffolo, Ruf\"\n",
      " Ruffo, Ruffolo, Ruffot, Ruffsner, Ruffstis, Ruffy, Ruff, Ru\n",
      "Epoch 8/10\n",
      "250884/250884 [==============================] - 144s 573us/step - loss: 0.7376\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Birk, Birkel, Birke\"\n",
      " Birk, Birkel, Birken, Birkens, Birkens, Birkens, Birkens, B\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Birk, Birkel, Birke\"\n",
      " Birk, Birkel, Birkell, Birkelin, Birken, Birker, Birkey, Bi\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Birk, Birkel, Birke\"\n",
      " Birk, Birkel, Birkelly, Birker, Birkey, Birkie, Birkin, Bir\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Birk, Birkel, Birke\"\n",
      " Birk, Birkel, Birkeli, Birkenion, Birker, Birkey, Birkitts,\n",
      "Epoch 9/10\n",
      "250884/250884 [==============================] - 153s 610us/step - loss: 0.7287\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ck, Boudoin, Boudrea\"\n",
      "ck, Boudoin, Boudrea, Bouds, Boudt, Boudt, Boudter, Boudthan\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ck, Boudoin, Boudrea\"\n",
      "ck, Boudoin, Boudreaum, Bouds, Boudt, Boudton, Boudtine, Bou\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ck, Boudoin, Boudrea\"\n",
      "ck, Boudoin, Boudreavy, Boudricki, Bouds, Boudy, Boudrey, Bo\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ck, Boudoin, Boudrea\"\n",
      "ck, Boudoin, Boudreale, Boue, Bouelah, Bouerd, Bouermeyt, Bo\n",
      "Epoch 10/10\n",
      "250884/250884 [==============================] - 149s 594us/step - loss: 0.7212\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"do, Milas, Milazzo, \"\n",
      "do, Milas, Milazzo, Mila, Milack, Milacci, Milacci, Milack, \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"do, Milas, Milazzo, \"\n",
      "do, Milas, Milazzo, Milas, Milassa, Milaster, Milasse, Milas\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"do, Milas, Milazzo, \"\n",
      "do, Milas, Milazzo, Milael, Milan, Miland, Milard, Milardo, \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"do, Milas, Milazzo, \"\n",
      "do, Milas, Milazzo, Mila, Milan, Milanek, Milard, Milarsbich\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b9175b518>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "%timeit -n1 -r1\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Milasse\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Milassa\" in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes around 20 minutes to run 10 epochs. Many names that the model generates don't sound coherent at all, but some sounds good, like \"Milasse\" or \"Milassa\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250884/250884 [==============================] - 132s 527us/step - loss: 1.2771\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"teer, Mateja, Matejk\"\n",
      "teer, Mateja, Matejka, Materie, Matery, Matery, Matery, Mate\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"teer, Mateja, Matejk\"\n",
      "teer, Mateja, Matejka, Mateh, Mather, Mathermen, Matherger, \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"teer, Mateja, Matejk\"\n",
      "teer, Mateja, Matejk, Mather, Matherzok, Matherytan, Mather,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"teer, Mateja, Matejk\"\n",
      "teer, Mateja, Matejked, Matinta, Mathku, Matky, Matkies, Mat\n",
      "Epoch 2/10\n",
      "250884/250884 [==============================] - 141s 563us/step - loss: 0.8790\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"akers, Rakes, Rakesh\"\n",
      "akers, Rakes, Rakesh, Rakes, Raki, Rakin, Rakle, Rakle, Rakl\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"akers, Rakes, Rakesh\"\n",
      "akers, Rakes, Rakesh, Rakes, Rakes, Rakick, Rakio, Rakin, Ra\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"akers, Rakes, Rakesh\"\n",
      "akers, Rakes, Rakesh, Rakew, Ral, Ralags, Raland, Ralana, Ra\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"akers, Rakes, Rakesh\"\n",
      "akers, Rakes, Rakesh, Rakofardo, Rakownl, Rakowski, Rakop, R\n",
      "Epoch 3/10\n",
      "250884/250884 [==============================] - 142s 564us/step - loss: 0.8236\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"hail, Macphee, Macph\"\n",
      "hail, Macphee, Macphell, Machmeyer, Machman, Machmann, Machm\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hail, Macphee, Macph\"\n",
      "hail, Macphee, Macphole, Macpine, Macpono, Macopins, Macorwe\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"hail, Macphee, Macph\"\n",
      "hail, Macphee, Macpha, Macree, Macreco, Macr, Macarra, Macar\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"hail, Macphee, Macph\"\n",
      "hail, Macphee, Macphmann, Macholin, Macholda, Mackom, Macksa\n",
      "Epoch 4/10\n",
      "250884/250884 [==============================] - 144s 573us/step - loss: 0.7948\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Ueda, Uehara, Uehlin\"\n",
      "Ueda, Uehara, Uehling, Uehler, Uehler, Uehler, Uehler, Uehle\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Ueda, Uehara, Uehlin\"\n",
      "Ueda, Uehara, Uehlinger, Ueiren, Ueirer, Uehl, Uhert, Uhard,\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Ueda, Uehara, Uehlin\"\n",
      "Ueda, Uehara, Uehling, Buelington, Bueller, Buepler, Bueper,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Ueda, Uehara, Uehlin\"\n",
      "Ueda, Uehara, Uehling, Uel, Bpelil, Bpeman, Scemero, Scemf, \n",
      "Epoch 5/10\n",
      "250884/250884 [==============================] - 169s 673us/step - loss: 0.7747\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Knitter, Knittle, Kn\"\n",
      "Knitter, Knittle, Knitz, Knitz, Knja, Knford, Knford, Knford\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Knitter, Knittle, Kn\"\n",
      "Knitter, Knittle, Knitz, Knitz, Knitz, Knjell, Knjer, Knjett\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Knitter, Knittle, Kn\"\n",
      "Knitter, Knittle, Knitz, Knox, Kney, Knean, Kneant, Knean, K\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Knitter, Knittle, Kn\"\n",
      "Knitter, Knittle, Knitz, Knja, Knfler, Knfaneppen, Knannee, \n",
      "Epoch 6/10\n",
      "250884/250884 [==============================] - 155s 619us/step - loss: 0.7617s - \n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"l, Oneill, Onette, O\"\n",
      "l, Oneill, Onette, Onette, Onette, Onette, Onette, Onette, O\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"l, Oneill, Onette, O\"\n",
      "l, Oneill, Onette, Onfelter, Onfelth, Onfelt, Onfield, Onfin\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"l, Oneill, Onette, O\"\n",
      "l, Oneill, Onette, Onete, Onfallyne, Onford, Ongsum, Onge, O\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"l, Oneill, Onette, O\"\n",
      "l, Oneill, Onette, Oney, Oncor, Oncol, Onco, Omd, Omddok, Om\n",
      "Epoch 7/10\n",
      "250884/250884 [==============================] - 134s 534us/step - loss: 0.7520\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"se, Heyser, Heyward,\"\n",
      "se, Heyser, Heyward, Heyward, Heyston, Heystein, Heystein, H\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"se, Heyser, Heyward,\"\n",
      "se, Heyser, Heyward, Heystein, Heyton, Heystein, Hey, Heya, \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"se, Heyser, Heyward,\"\n",
      "se, Heyser, Heyward, Heyszel, Heytzel,k,kor, Hyba, Hybay, Hy\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"se, Heyser, Heyward,\"\n",
      "se, Heyser, Heyward, Hez, Heze, Hczek, Htbelin, Stluff, Stlu\n",
      "Epoch 8/10\n",
      "250884/250884 [==============================] - 146s 583us/step - loss: 0.7426\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", Mcgoey, Mcgoff, Mc\"\n",
      ", Mcgoey, Mcgoff, Mcgor, Mcgorn, Mcgorney, Mcgorn, Mcgorney,\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", Mcgoey, Mcgoff, Mc\"\n",
      ", Mcgoey, Mcgoff, Mcgorger, Mcgorn, Mcgorney, Mcgorner, Mcgo\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", Mcgoey, Mcgoff, Mc\"\n",
      ", Mcgoey, Mcgoff, Mcgeb, Mcgeehall, Mcgeill, Mcgeing, Mcgeir\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", Mcgoey, Mcgoff, Mc\"\n",
      ", Mcgoey, Mcgoff, Mcgoetgen, Mcgodl, Mcgobgill, Mcirblett, M\n",
      "Epoch 9/10\n",
      "250884/250884 [==============================] - 138s 551us/step - loss: 0.7347\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"zwelder, Swasey, Swa\"\n",
      "zwelder, Swasey, Swaess, Swaez, Sweiger, Sweiler, Sweill, Sw\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"zwelder, Swasey, Swa\"\n",
      "zwelder, Swasey, Swaes, Sweid, Sweel, Sweeler, Sweeler, Swee\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"zwelder, Swasey, Swa\"\n",
      "zwelder, Swasey, Swalbert, Swaan, Swale, Swalbert, Swalbert,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"zwelder, Swasey, Swa\"\n",
      "zwelder, Swasey, Swanter, Swanra, Swerkler, Swerdler, Swert,\n",
      "Epoch 10/10\n",
      "250884/250884 [==============================] - 141s 560us/step - loss: 0.7304\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Thuan, Thul, Thulin,\"\n",
      "Thuan, Thul, Thulin, Thul, Thul, Thul, Thumb, Thumm, Thump, \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Thuan, Thul, Thulin,\"\n",
      "Thuan, Thul, Thulin, Thul, Thul, Thum, Thun, Thuna, Thunt, T\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Thuan, Thul, Thulin,\"\n",
      "Thuan, Thul, Thulin, Thuki, Thuna, Thunen, Thunt, Thunt, Thu\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Thuan, Thul, Thulin,\"\n",
      "Thuan, Thul, Thulin, Thullac, Thulze, Thum, Thune, Thuns, Th\n",
      "24min 13s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "%timeit -n1 -r1 model.fit(x, y,batch_size=128,epochs=10,callbacks=[print_callback], shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Heystein\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Mcgorney\" in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes somewhat longer, but not too bad. The coherency doesn't seem to improve..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
